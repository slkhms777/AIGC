\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{geometry}
\geometry{a4paper, margin=2.5cm}
\renewcommand{\baselinestretch}{1.5}

\title{Introduce to Artificial Intelligence Homework}
\author{}
\date{}

\begin{document}

\maketitle

\section*{Abstract}

With the popularity of AI recently, more and more AI/CV researchers have attached a great importance to 
generative model-based image generation/synthesis technologies (In the following text, it will be uniformly 
abbreviated as ``text to image'', i.e., T2I). This article will focus on natural language-driven image 
generation/synthesis models and roughly summarize the previous related work and latest advances in this field.

\vspace{1.5cm}

\section*{Introduction}

Nowadays, the technologies of image generation/synthesis are a crucial research direction in computer vision 
field. With the surge of artificial intelligence generated content and especially deep learning-driven 
generation models' boosting, the technologies of image generation/synthesis have reached an unprecedented 
level. These technologies can not only create original static images but also videos and 3D content. Then I 
will introduce the history of generation models including previous work (AE and its variants, including DAE, 
VAE, VQ-VAE as well as diffusion model series) and latest advances (OpenAI DALL·E series and Google Imagen 
series).

\vspace{1.5cm}

\noindent\textbf{Project Repository:} \url{https://github.com/slkhms777/AIGC}

\newpage
\section{Previous Related Work}

\subsection{GAN}

This work was proposed by Ian Goodfellow et al. in 2014, with its core innovation being in the firstly 
introduction of the ``adversarial learning'' mechanism. (I have read Ian's book on deep learning, so he left 
a deep impression on me). The \textbf{GAN} model has two main parts: one is the \textit{Generator} and the 
other is \textit{Discriminator}. The objective of \textit{Generator} is to learn the distribution of real 
data and generate fake data that are as realistic as possible (for example, image) while the objective of 
\textit{Discriminator} is to determine whether the images are real data or fake data generated by 
\textit{Generator}. The ultimate goal of the \textbf{GAN} model is to train these two parts so that the 
\textit{Generator} has the ability to generate fake images that cannot be distinguished by 
\textit{Discriminator}.

\textbf{Advantages}
\begin{itemize}
    \item High-quality data generation
    \item No explicit probability modeling required
\end{itemize}

\textbf{Disadvantages}
\begin{itemize}
    \item Training instability
    \item High data requirement
    \item Lack of image diversity
\end{itemize}

\subsubsection{Wasserstein GAN (WGAN)}
WGAN addresses the training instability of vanilla GANs by introducing the Wasserstein distance (Earth Mover's Distance) as a new loss metric. This approach provides smoother gradients and alleviates mode collapse. WGAN replaces the discriminator with a "critic" that scores the realness of samples, and enforces a Lipschitz constraint, typically via weight clipping or gradient penalty (WGAN-GP). As a result, WGANs are more stable and easier to train, especially on complex datasets.

\subsubsection{Conditional GAN (CGAN)}
CGAN extends the GAN framework by conditioning both the generator and discriminator on auxiliary information, such as class labels or attributes. This allows the model to generate images corresponding to specific categories or features, enabling controlled image synthesis. The conditioning is usually implemented by concatenating the label information with the input noise vector and/or the input to the discriminator.

\subsubsection{BigGAN}
BigGAN, proposed by DeepMind in 2018, scales up the GAN architecture to achieve state-of-the-art image synthesis on large and complex datasets like ImageNet. BigGAN uses larger batch sizes, deeper networks, and advanced regularization techniques. It also incorporates class-conditional information and spectral normalization to stabilize training. BigGAN demonstrates that scaling up GANs leads to significant improvements in image fidelity and diversity, but also requires careful tuning and substantial computational resources.

\subsection{AE Series}

Due to the information density of images is low, there is a significant pixel redundancy. For example, the 
concept of ``a cat'' can be expressed in natural language with just a few words, whereas representing it as 
an RGB image requires hundreds or thousands of pixels. And a work named \textbf{MAE} proposed by Kaiming He 
et al. in 2021 has proved that if an image is masked by 75\% region, it can still be roughly reconstructed 
by a transformer-based image model. This further demonstrates that images are a low-information density 
modality.

Under this prior condition, AE series' main algorithm is to generate an image from a latent vector, whose 
dimensionality is much lower than the original image.

\subsubsection{AutoEncoder (AE)}

The inputs for \textbf{AE} are the original images. Each image is firstly encoded into a latent vector by an 
encoder, and then decoded into a reconstructed image by a decoder. The loss function used is mean squared 
error (MSE) Loss.

\subsubsection{Denoising AutoEncoder (DAE)}

The main difference between an \textbf{AutoEncoder (AE)} and a \textbf{Denoising AutoEncoder (DAE)} is that, 
in DAE, noise is deliberately added to the input images during training, and the model is trained to 
reconstruct the original, noise-free images from these corrupted inputs. Hence the name, \textbf{DAE}.

Though \textbf{AE} and \textbf{DAE} are good at reconstructing the original images, reconstruction is the 
only thing they can do. The latent vector, also called bottleneck, is not modeled as a probability 
distribution. Therefore, it cannot be randomly sampled to generate new images.

\subsubsection{Variational AutoEncoder (VAE)}

To solve the disadvantages of \textbf{AE} and \textbf{DAE}, \textbf{Variational AutoEncoder} not only 
compresses the input image into a latent vector, but also requires that these latent vectors follow a 
probability distribution (usually a standard normal distribution), while retaining the well-designed 
encoder-decoder architecture of \textbf{AE} and \textbf{DAE}.

It should be noted that the loss of a VAE contains not only MSE loss but also KL loss. Specifically, the KL 
loss is calculated as:
\begin{equation}
\text{KL\_loss} = -0.5 \times \sum \left(1 + \log(\sigma^2) - \mu^2 - \sigma^2\right)
\end{equation}
Note that $\mu$ and $\sigma^2$ represent the mean and variance, respectively.


\textbf{VAE} has many advantages, such as being able to generate new data samples rather than just 
reconstructing the input, and providing a clear probabilistic interpretation. However, it also has some 
drawbacks: on the other hand, since \textbf{VAE} models the latent space using continuous probability 
distributions, it cannot effectively handle discrete data. At the same time, when dealing with more complex 
or large-scale data, the scalability of the model can be limited, and the quality of generated samples is 
sometimes inferior to that of other generative models.

\subsubsection{Vector Quantized Variational AutoEncoder (VQ-VAE)}

\textbf{VQ-VAE} uses a codebook to discretize the latent space, instead of modeling it with a normal 
distribution as in a standard \textbf{VAE}. This approach allows the features produced by the encoder to be 
quantized into discrete representations. While this enables effective compression and discrete modeling, 
\textbf{VQ-VAE} alone cannot directly generate new images. To sample or generate new images, an additional 
prior network (such as PixelCNN or Transformer) is required to model the distribution over the discrete 
latent variables. Without such a prior, \textbf{VQ-VAE} can only be used for tasks like reconstruction or 
feature extraction, rather than true generative modeling.

It should be noted that the loss function of a \textbf{VQ-VAE} consists of three parts: the reconstruction 
loss (MSELoss), the codebook loss, and the commitment loss.
\begin{equation}
\text{VQ-VAE\_loss} = \text{recon\_loss} + \text{vq\_loss} + \beta \times \text{commit\_loss}
\end{equation}

For its \textbf{Prior} model, the loss function is CrossEntropyLoss, since it is an autoregressive model.
\begin{equation}
\text{Prior\_loss} = \text{CrossEntropyLoss}
\end{equation}

\subsection{Diffusion Series}

\subsubsection{Diffusion}

The original diffusion models are defined as follows:

\textbf{Forward diffusion:} Given an image, a small amount of noise is added at each step, for a total of T 
steps. If T is very large, the image will eventually become pure noise, specifically isotropic Gaussian 
noise.

\textbf{Reverse diffusion:} Starting from random noise, a neural network is used to gradually remove the 
noise, step by step, until an image is reconstructed.

Because T is typically quite large (e.g., T = 1000 in the original diffusion models), both training and 
inference of diffusion models are much more time-consuming than models like GANs, as multiple forward passes 
are required.

To maintain the same input and output image size during the denoising process, the original diffusion model 
adopts a shared-parameter U-Net, which is an encoder-decoder structure. To further improve image 
reconstruction quality, skip connections and attention mechanisms were introduced into the U-Net.

Although the concept of diffusion models was proposed over a decade ago, it wasn't until 2020, with the 
introduction of DDPM, that diffusion models began to gain widespread attention.

\subsubsection{DDPM}

DDPM (Denoising Diffusion Probabilistic Models) introduced two main innovations:

\begin{enumerate}
    \item \textbf{Noise prediction:} In the denoising process, instead of directly predicting the clean image 
    at each step, DDPM predicts the added noise (analogous to the residual in ResNet). Additionally, time 
    embeddings are incorporated into the U-Net, which helps the model generate and sample images more 
    effectively. This works because the U-Net uses shared parameters, and time-step embeddings enable the 
    model to produce step-specific outputs. For example, in early denoising steps, the model is expected to 
    generate only the coarse outline of objects, while finer details and high-frequency features are 
    gradually restored as the steps progress.
    \item \textbf{Variance simplification:} While predicting a Gaussian distribution typically requires 
    modeling both the mean and variance, DDPM shows that it suffices to predict only the mean, with the 
    variance set as a constant. This simplification not only maintains strong performance but also eases 
    model optimization.
\end{enumerate}

\subsubsection{Improved DDPM}

OpenAI made several improvements to DDPM, with three key contributions:

\begin{enumerate}
    \item \textbf{Learnable variance:} While the original DDPM used a fixed variance, the OpenAI team made 
    the variance a learnable parameter, which improved the quality of generated and sampled images.
    \item \textbf{Noise schedule optimization:} The noise addition schedule was changed from a linear 
    schedule to a cosine schedule (similar to how learning rate schedules work), resulting in significantly 
    better performance.
    \item \textbf{Scalability and classifier guidance:} OpenAI demonstrated that DDPM scales very well with 
    large and complex models. In the "Diffusion Beats GAN" paper (\textbf{DALL-E 2}'s second and third 
    authors), they increased the model's depth and width, and added more self-attention heads. Since 
    single-scale attention was insufficient, they introduced multi-scale attention, making the models both 
    larger and more complex. Furthermore, they proposed a classifier guidance method, which reduced the 
    number of sampling steps to around 25. These innovations laid the foundation for subsequent research in 
    this field.
\end{enumerate}

Since then, diffusion models have risen to prominence and become one of the hottest research directions in 
image generation.

\subsubsection{DDIM}

DDIM (Denoising Diffusion Implicit Models) is a significant improvement over DDPM that dramatically accelerates
the sampling process. While DDPM relies on a stochastic Markov chain for denoising, requiring hundreds or thousands 
of steps, DDIM introduces a non-Markovian deterministic sampling process that allows for high-quality image 
generation with much fewer steps (e.g., 25 or 50 steps).

The main innovations of DDIM include:

\begin{enumerate}
    \item \textbf{Deterministic Sampling:} DDIM reparameterizes the reverse process, enabling deterministic 
    results from the same starting noise, which improves reproducibility and controllability of the results.
    \item \textbf{Accelerated Inference:} By skipping intermediate time steps, DDIM can dramatically reduce 
    sampling time while maintaining image quality, making real-time applications feasible.
    \item \textbf{Flexible Sampling Strategy:} DDIM supports interpolation between deterministic and 
    stochastic sampling, allowing users to control the degree of randomness in generation by adjusting parameters.
\end{enumerate}

The mathematical formulation of DDIM is based on the key observation that the original DDPM forward process can 
be viewed as a special case, while DDIM provides a more general framework. By redefining the conditional distributions 
of the reverse process, DDIM can achieve generation effects similar to or even better than DDPM with fewer steps.

Overall, DDIM provides a more efficient and flexible solution for diffusion-based generative modeling, becoming 
the foundation for many subsequent diffusion model improvements.

\subsubsection{Classifier Guidance}

\begin{enumerate}
    \item Let's focus on the transition from $X_t$ to $X_{t-1}$. If we add a large amount of noise to images 
    from ImageNet and use these noisy images to train a classifier, the resulting classifier can facilitate 
    the denoising process for the diffusion model. More generally, before each denoising step, we can use 
    the classifier to make a prediction, obtain its gradient with respect to the input, and use this 
    gradient to guide the diffusion process in the correct direction during parameter updates.
    \item Following the success of classifier guidance, some researchers began to use a CLIP model as the 
    guidance signal instead of a traditional classifier. This approach proved to be very effective, allowing 
    both text and images to serve as guidance signals. Different guidance signals lead to different types of 
    generation tasks, including text-driven image generation, image reconstruction, and image style transfer.
\end{enumerate}

\subsubsection{Classifier-free Guidance}

If we assume the guidance signal is text, then during training, the model needs to perform forward passes 
both with and without the conditioning signal. In this way, the model learns the impact that the text signal 
has on the output. Although this method is computationally expensive, its effectiveness cannot be ignored. 
After being proposed, this approach was adopted in \textbf{GLIDE}, \textbf{DALL-E 2} and \textbf{Imagen}.

\subsubsection{GLIDE}

\textbf{GLIDE} incorporates techniques such as DDPM, improved DDPM, and classifier-free guidance, and is a 
diffusion model with 3.5 billion parameters. It later evolved into DALL·E 2.

\subsection{DALL-E series}

Originally, \textbf{VQ-VAE} used \textbf{PixelCNN} as its prior for sampling or generating new images, taking 
advantage of \textbf{PixelCNN}'s autoregressive nature. However, OpenAI later replaced \textbf{PixelCNN} 
with their signature autoregressive model, \textbf{GPT}, to further enhance the quality and flexibility of 
image generation.

\subsubsection{DALL-E}

The \textbf{DALL-E} model, developed by OpenAI, is a text-to-image generation model. During training, the 
text prompt is first encoded into text tokens, and the image is encoded into image tokens using a 
\textbf{VQ-VAE} encoder. These two sequences of tokens are concatenated and fed into a \textbf{GPT} model, 
which learns to perform autoregressive modeling over the combined sequence.
During inference, the process is simpler: the text prompt is encoded into text tokens, which are then passed 
to the \textbf{GPT} model to autoregressively predict the corresponding image tokens. Finally, these image 
tokens are decoded by the \textbf{VQ-VAE} decoder to generate a pixel-level image.

\subsubsection{DALL-E 2}

The training datasets of \textbf{DALL-E 2} consist of pairs $(x, y)$ of images and their corresponding 
captions $y$. Given an image $x$, let $z_i$ and $z_t$ be its CLIP image and text embeddings, respectively.

\begin{equation}
P(z_t | y) = \text{Prior}(y) \quad \text{and} \quad P(x | z_t, y) = \text{Decoder}(z_t, y)
\end{equation}
The generative stack uses two components to produce images from captions: a \textbf{prior} model and a 
\textbf{decoder} model. The prior produces CLIP image embeddings $z_i$ conditioned on caption $y$. The 
decoder produces images $x$ conditioned on CLIP image embeddings $z_i$ and text caption $y$.

\begin{equation}
P(x|y) = P(x, z_i | y) = P(x|z_i, y) P(z_i|y).
\end{equation}

The DALL-E 2 model uses CLIP guidance and classifier-free guidance to improve the quality of generated 
images but with a high computational cost.
To generate high resolution images, DALL-E 2 uses two diffusion upsample models.
There 2 choices for prior model: a diffusion prior model ans a autoregressive prior model. 
For DALL-E 2, directly predicting the unnoised $z_i$ is better than predicting the noise $\epsilon$.
Maybe the reason is that the target $z_i$ is more suitable for the DALL-E 2 model's architecture.

\subsection{Imagen}

\textbf{Imagen} is a text-to-image diffusion model proposed by Google Research, which achieves 
state-of-the-art performance on several image generation benchmarks. The core idea of Imagen is to leverage 
large pre-trained language models for text encoding and combine them with a powerful diffusion-based image 
decoder.

Unlike DALL-E 2, which uses CLIP for both text and image embeddings, Imagen utilizes a large frozen T5 
language model to encode the text prompt, arguing that high-quality language understanding is crucial for 
text-to-image generation. The text embedding produced by T5 is then fed into a series of diffusion models to 
generate images.

The overall pipeline of Imagen consists of three main components:
\begin{enumerate}
    \item \textbf{Text Encoder:} A large pre-trained T5 model is used to encode the input text prompt into a 
    dense embedding.
    \item \textbf{Base Diffusion Model:} The text embedding is concatenated with noise and passed into a 
    U-Net-based diffusion model to generate a low-resolution image (e.g., $64 \times 64$).
    \item \textbf{Super-Resolution Diffusion Models:} Two cascaded diffusion models are used to progressively 
    upsample the image to higher resolutions (e.g., $256 \times 256$ and $1024 \times 1024$), each 
    conditioned on the text embedding and the previous lower-resolution image.
\end{enumerate}

Imagen introduces several key innovations: Firstly, Instead of using cross-attention, Imagen injects the text 
embedding into the diffusion U-Net via a simple concatenation and FiLM (Feature-wise Linear Modulation) 
layers, which improves both efficiency and performance. Secondly, similar to GLIDE and DALL-E 2, Imagen adopts classifier-free 
guidance to further enhance the fidelity and relevance of generated images.

Experimental results show that Imagen achieves unprecedented photorealism and language-image alignment, 
outperforming previous models such as DALL-E 2 and GLIDE on various benchmarks. 


\subsection{Summary of previous work}
These work are all proposed before 2022.10 when the chatgpt firstly released. But they totally proved:
\begin{enumerate}
    \item \textbf{Classifier-free guidance} is the answer. It is a very useful technique to improve the performance of generative models.
    \item \textbf{Diffusion model} is the answer. It is a very powerful generative model, which can be used to high-quality images 
    with both variability and fidelity.
    \item \textbf{Scaling} is the answer. Whatever the architecture or the training strategy or the loss function, the generative models are all 
    need to "Scale up" to get the better performance.
\end{enumerate}


\section{Latest Advances}

Since late 2022, the field of text-to-image generation has witnessed rapid and transformative advances, driven by both academic research and industry applications. Below, we summarize several of the most influential developments:

\subsection{Stable Diffusion and the Open-Source Revolution}
Stable Diffusion, released by Stability AI in August 2022, marked a turning point by making high-quality text-to-image diffusion models accessible to the public. Unlike previous proprietary models, Stable Diffusion is fully open-source, allowing researchers, developers, and artists to fine-tune, deploy, and build upon the model freely. Its lightweight architecture enables efficient inference on consumer GPUs, greatly lowering the barrier to entry. The open-source ecosystem around Stable Diffusion has fostered rapid innovation, including community-driven improvements, plug-ins, and creative applications.

\subsection{Midjourney and Community-Driven Innovation}
Midjourney is a commercial text-to-image platform that has gained popularity for its unique artistic style and user-friendly interface. By leveraging Discord as its primary interaction channel, Midjourney has built a vibrant community where users can collaboratively explore prompt engineering and share results. The model is known for its stylized, imaginative outputs, and frequent updates based on user feedback.

\subsection{DALL\textperiodcentered E 3 and Prompt Engineering}
OpenAI's DALL\textperiodcentered E 3, released in 2023, further improved the fidelity, coherence, and controllability of generated images. DALL\textperiodcentered E 3 is notable for its deep integration with large language models (LLMs), enabling users to describe complex scenes in natural language and receive highly relevant images. The model demonstrates strong prompt following and can handle nuanced instructions, making prompt engineering a critical skill for users.

\subsection{Imagen Editor and Controllable Generation}
Google's Imagen Editor extends the Imagen family by introducing fine-grained, interactive image editing capabilities. Users can specify edits using natural language, such as modifying objects, styles, or backgrounds within an image. This controllability is achieved by conditioning the diffusion process on both the original image and the edit instructions, opening new possibilities for creative workflows and content customization.

\subsection{Multimodal Large Models: GPT-4V, Gemini, and Beyond}
The latest trend is the emergence of multimodal large models, such as OpenAI's GPT-4V and Google's Gemini, which can process and generate both text and images (and even audio or video). These models unify language and vision understanding, enabling tasks like visual question answering, image captioning, and cross-modal reasoning. Their generalization ability and scalability are pushing the boundaries of what generative AI can achieve.

\subsection{Future Trends and Challenges}
Despite remarkable progress, several challenges remain: improving controllability, reducing biases, ensuring safety, and scaling to higher resolutions and more modalities. The integration of generative models with real-world applications (e.g., design, education, entertainment) is expected to accelerate, while ethical and societal considerations will become increasingly important.


\end{document}







